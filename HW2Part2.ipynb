{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW2Part2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/Colab\\ Notebooks/neural_machine_translation/rnn.py /content\n",
        "!cp /content/drive/MyDrive/Colab\\ Notebooks/neural_machine_translation/data_utils.py /content"
      ],
      "metadata": {
        "id": "APIOVZncdNTs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir data-bin\n",
        "!mkdir model\n",
        "!mkdir data"
      ],
      "metadata": {
        "id": "GgqFgup8f0QA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/Colab\\ Notebooks/neural_machine_translation/data-bin/* /content/data-bin\n",
        "!cp /content/drive/MyDrive/Colab\\ Notebooks/neural_machine_translation/model/* /content/model\n",
        "!cp /content/drive/MyDrive/Colab\\ Notebooks/neural_machine_translation/data/* /content/data\n"
      ],
      "metadata": {
        "id": "0Ss6OQuMfJre"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "import string\n",
        "import random\n",
        "from data_utils import *\n",
        "from rnn import *\n",
        "import torch\n",
        "import codecs\n",
        "from tqdm import tqdm\n",
        "import string\n",
        "\n",
        "#Set GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#Load vocabulary files\n",
        "input_lang = torch.load('data-bin/fra.data')\n",
        "output_lang = torch.load('data-bin/eng.data')\n",
        "\n",
        "#Create and empty RNN model\n",
        "encoder = EncoderRNN(input_size=input_lang.n_words, device=device)\n",
        "attn_decoder = AttnDecoderRNN(output_size=output_lang.n_words, device=device)\n",
        "\n",
        "#Load the saved model weights into the RNN model\n",
        "encoder.load_state_dict(torch.load('model/encoder'))\n",
        "attn_decoder.load_state_dict(torch.load('model/decoder'))\n",
        "\n",
        "#Return the decoder output given input sentence \n",
        "#Additionally, the previous predicted word and previous decoder state can also be given as input\n",
        "def translate_single_word(encoder, decoder, sentence, decoder_input=None, decoder_hidden=None, max_length=MAX_LENGTH, device=device):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence, device)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        \n",
        "        encoder = encoder.to(device)\n",
        "        decoder = decoder.to(device)\n",
        "        \n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        if decoder_input==None:\n",
        "            decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "        else:\n",
        "            decoder_input = torch.tensor([[output_lang.word2index[decoder_input]]], device=device) \n",
        "        \n",
        "        if decoder_hidden == None:        \n",
        "            decoder_hidden = encoder_hidden\n",
        "        \n",
        "        decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "        return decoder_output.data, decoder_hidden\n",
        "\n",
        "#########################################################################################\n",
        "#####Modify this function to use beam search to predict instead of greedy prediction#####\n",
        "#########################################################################################\n",
        "def beam_search(encoder,decoder,input_sentence,beam_size = 1 ,max_length=MAX_LENGTH):\n",
        "    decoded_output = []\n",
        "    \n",
        "    #Predicted the first word\n",
        "    decoder_output, decoder_hidden = translate_single_word(encoder, decoder, input_sentence, decoder_input=None, decoder_hidden=None)\n",
        "    \n",
        "    #Get the probability of all output words\n",
        "    decoder_output_probs = decoder_output.data\n",
        "    \n",
        "    #Select the id of the word with maximum probability\n",
        "    idx = torch.argmax(decoder_output_probs)\n",
        "\t\n",
        "    #Convert the predicted id to the word\n",
        "    first_word = output_lang.index2word[idx.item()]\n",
        "    \n",
        "    #Add the predicted word to the output list and also set it as the previous prediction\n",
        "    decoded_output.append(first_word)\n",
        "    previous_decoded_output = first_word\n",
        "    \n",
        "    #Loop until the maximum length\n",
        "    for i in range(max_length):\n",
        "    \n",
        "        #Predict the next word given the previous prediction and the previous decoder hidden state\n",
        "        decoder_output, decoder_hidden = translate_single_word(encoder, decoder, input_sentence, previous_decoded_output, decoder_hidden)\n",
        "        \n",
        "        #Get the probability of all output words\n",
        "        decoder_output_probs = decoder_output.data\n",
        "        \n",
        "        #Select the id of the word with maximum probability\n",
        "        idx = torch.argmax(decoder_output_probs)\n",
        "        \n",
        "        #Break if end of sentence is predicted\n",
        "        if idx.item() == EOS_token:\n",
        "            break \n",
        "            \n",
        "        #Else add the predicted word to the list\n",
        "        else:\n",
        "            #Convert the predicted id to the word\n",
        "            selected_word = output_lang.index2word[idx.item()]\n",
        "            \n",
        "            #Add the predicted word to the output list and update the previous prediction\n",
        "            decoded_output.append(selected_word)    \n",
        "            previous_decoded_output = selected_word\n",
        "            \n",
        "    #Convert list of predicted words to a sentence and detokenize \n",
        "    output_translation = \" \".join(i for i in decoded_output)\n",
        "    \n",
        "    return output_translation\n",
        "\n",
        "\n",
        "# target_sentences = [\"i can speak a bit of french .\",\n",
        "#         \"i ve bought some cheese and milk .\",\n",
        "#         \"boy where is your older brother ?\",\n",
        "#         \"i ve just started reading this book .\",\n",
        "#         \"she loves writing poems .\"]\n",
        "\n",
        "target_sentences = []\n",
        "\n",
        "with open('data/test.eng') as targetS:\n",
        "    for line in targetS:\n",
        "        target_sentences.append(line)\n",
        "\n",
        "\n",
        "# source_sentences = [\"je parle un peu francais .\",\n",
        "#             \"j ai achete du fromage et du lait .\",\n",
        "#             \"garcon ou est ton grand frere ?\",\n",
        "#             \"je viens justement de commencer ce livre .\",\n",
        "#             \"elle adore ecrire des poemes .\"]\n",
        "\n",
        "source_sentences = []\n",
        "\n",
        "with open('data/test.fra') as sourceS:\n",
        "    for line in sourceS:\n",
        "        source_sentences.append(line)\n",
        "\n",
        "\n",
        "\n",
        "target = codecs.open('example.txt','w',encoding='utf-8')\n",
        "test_out = codecs.open('test_beam_1.out','w',encoding='utf-8')\n",
        "\n",
        "beam_size = 1 \n",
        "for i,source_sentence in enumerate(source_sentences):\n",
        "\n",
        "    target_sentence = normalizeString(target_sentences[i])\n",
        "    input_sentence = normalizeString(source_sentence)\n",
        "    \n",
        "    hypothesis = beam_search(encoder, attn_decoder, input_sentence, beam_size=beam_size)\n",
        "    \n",
        "    # print(\"S-\"+str(i)+\": \"+input_sentence)\n",
        "    # print(\"T-\"+str(i)+\": \"+target_sentence)\n",
        "    # print(\"H-\"+str(i)+\": \"+hypothesis)\n",
        "    # print()\n",
        "    target.write(hypothesis+'\\n')\n",
        "\n",
        "    # test_out.write(\"S-\"+str(i)+\": \"+input_sentence+'\\n')\n",
        "    # test_out.write(\"T-\"+str(i)+\": \"+target_sentence+'\\n')\n",
        "    test_out.write(hypothesis+'\\n')\n",
        "\n",
        "target.close()\n",
        "test_out.close()    \n"
      ],
      "metadata": {
        "id": "7lmkdVz7dqL9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sacrebleu "
      ],
      "metadata": {
        "id": "slRMjL8tgpJc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5b28d94-3f39-458f-a239-a9533636c151"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (2.4.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (0.8.9)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (0.4.4)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from sacrebleu) (1.21.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sacrebleu data/test.eng < test_beam_1.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9bjc5CzlgFt",
        "outputId": "d0790291-c658-4ad8-881c-a764b2e14311"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sacreBLEU: That's 100 lines that end in a tokenized period ('.')\n",
            "sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 13.0,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.0.0\",\n",
            " \"verbose_score\": \"41.5/13.8/8.3/6.0 (BP = 1.000 ratio = 1.037 hyp_len = 6446 ref_len = 6218)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.0.0\"\n",
            "}\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "import string\n",
        "import random\n",
        "from data_utils import *\n",
        "from rnn import *\n",
        "import torch\n",
        "import codecs\n",
        "from tqdm import tqdm\n",
        "import string\n",
        "import sacrebleu\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Set GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#Load vocabulary files\n",
        "input_lang = torch.load('data-bin/fra.data')\n",
        "output_lang = torch.load('data-bin/eng.data')\n",
        "\n",
        "#Create and empty RNN model\n",
        "encoder = EncoderRNN(input_size=input_lang.n_words, device=device)\n",
        "attn_decoder = AttnDecoderRNN(output_size=output_lang.n_words, device=device)\n",
        "\n",
        "#Load the saved model weights into the RNN model\n",
        "encoder.load_state_dict(torch.load('model/encoder'))\n",
        "attn_decoder.load_state_dict(torch.load('model/decoder'))\n",
        "\n",
        "#Return the decoder output given input sentence \n",
        "#Additionally, the previous predicted word and previous decoder state can also be given as input\n",
        "def translate_single_word(encoder, decoder, sentence, decoder_input=None, decoder_hidden=None, max_length=MAX_LENGTH, device=device):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence, device)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        \n",
        "        encoder = encoder.to(device)\n",
        "        decoder = decoder.to(device)\n",
        "        \n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        if decoder_input==None:\n",
        "            decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "        else:\n",
        "            decoder_input = torch.tensor([[output_lang.word2index[decoder_input]]], device=device) \n",
        "        \n",
        "        if decoder_hidden == None:        \n",
        "            decoder_hidden = encoder_hidden\n",
        "        \n",
        "        decoder_output, decoder_hidden, decoder_attention = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "        return decoder_output.data, decoder_hidden\n",
        "\n",
        "#########################################################################################\n",
        "#####Modify this function to use beam search to predict instead of greedy prediction#####\n",
        "#########################################################################################\n",
        "def beam_search(encoder,decoder,input_sentence,beam_size = 1 ,max_length=MAX_LENGTH):\n",
        "    decoded_output = []\n",
        "    \n",
        "    #Predicted the first word\n",
        "    decoder_output, decoder_hidden = translate_single_word(encoder, decoder, input_sentence, decoder_input=None, decoder_hidden=None)\n",
        "    \n",
        "    #Get the probability of all output words\n",
        "    decoder_output_probs = decoder_output.data\n",
        "    \n",
        "    #Select the id of the word with maximum probability\n",
        "    idx = torch.argmax(decoder_output_probs)\n",
        "    previousWinProbability = decoder_output_probs[0][idx] # will be used for the beam search\n",
        "\n",
        "    #Convert the predicted id to the word\n",
        "    first_word = output_lang.index2word[idx.item()]\n",
        "    \n",
        "    #Add the predicted word to the output list and also set it as the previous prediction\n",
        "    decoded_output.append(first_word)\n",
        "    previous_decoded_output = first_word\n",
        "    \n",
        "    #Loop until the maximum length\n",
        "    for i in range(max_length):\n",
        "    \n",
        "        #Predict the next word given the previous prediction and the previous decoder hidden state\n",
        "        decoder_output, decoder_hidden = translate_single_word(encoder, decoder, input_sentence, previous_decoded_output, decoder_hidden)\n",
        "        \n",
        "        #Get the probability of all output words\n",
        "        decoder_output_probs = decoder_output.data\n",
        "        \n",
        "\n",
        "        #Select the id of the word with maximum probability\n",
        "        #xxx idx = torch.argmax(decoder_output_probs)\n",
        "\n",
        "        # *************** sort all new sequences in the descending order of their score ***************\n",
        "        sorted_decoder_output_probs = torch.sort(decoder_output_probs, descending=True)\n",
        "        idxList = []\n",
        "        probabilityList = []\n",
        "        #xxx get top n (beam-size) scores\n",
        "        for x in range(beam_size):\n",
        "            idxList.append(sorted_decoder_output_probs[1][0][x])\n",
        "            idx = sorted_decoder_output_probs[1][0][x]\n",
        "            probabilityList.append((sorted_decoder_output_probs[0][0][x]) * previousWinProbability)\n",
        "            selected_word = output_lang.index2word[idx.item()]\n",
        "            previous_decoded_output = selected_word\n",
        "            if previous_decoded_output != 'EOS':\n",
        "                decoder_output, decoder_hidden = translate_single_word(encoder, decoder, input_sentence, previous_decoded_output, decoder_hidden)\n",
        "        \n",
        "        previousWinProbability = max(probabilityList)\n",
        "        idx_max = max(range(len(probabilityList)), key=probabilityList.__getitem__)\n",
        "        idx = idxList[idx_max]\n",
        "        \n",
        "        #Break if end of sentence is predicted\n",
        "        if idx.item() == EOS_token:\n",
        "            break \n",
        "            \n",
        "        #Else add the predicted word to the list\n",
        "        else:\n",
        "            #Convert the predicted id to the word\n",
        "            selected_word = output_lang.index2word[idx.item()]\n",
        "            \n",
        "            #Add the predicted word to the output list and update the previous prediction\n",
        "            decoded_output.append(selected_word)    \n",
        "            previous_decoded_output = selected_word\n",
        "            \n",
        "    #Convert list of predicted words to a sentence and detokenize \n",
        "    output_translation = \" \".join(i for i in decoded_output)\n",
        "    \n",
        "    return output_translation\n",
        "\n",
        "\n",
        "target_sentences = []\n",
        "\n",
        "with open('data/valid.eng') as targetS:\n",
        "    for line in targetS:\n",
        "        target_sentences.append(line)\n",
        "\n",
        "source_sentences = []\n",
        "\n",
        "with open('data/valid.fra') as sourceS:\n",
        "    for line in sourceS:\n",
        "        source_sentences.append(line)\n",
        "\n",
        "bleu_scores = []\n",
        "beam_values = []\n",
        "beamSizeMax = 19\n",
        "for x in range(beamSizeMax):\n",
        "    beam_size = x+1\n",
        "    beam_values.append(beam_size)\n",
        "    predictions = []\n",
        "    print(str(beam_size))\n",
        "    for i,source_sentence in enumerate(source_sentences):\n",
        "        target_sentence = normalizeString(target_sentences[i])\n",
        "        input_sentence = normalizeString(source_sentence)\n",
        "        hypothesis = beam_search(encoder, attn_decoder, input_sentence, beam_size=beam_size)\n",
        "    \n",
        "        predictions.append(hypothesis)\n",
        "  \n",
        "    \n",
        "    bleu = sacrebleu.corpus_bleu(predictions, target_sentences)\n",
        "    bleu_scores.append(bleu.score)\n",
        "\n",
        "plt.plot(beam_values, bleu_scores)\n",
        "\n",
        "# naming the x axis\n",
        "plt.xlabel('beam size')\n",
        "# naming the y axis\n",
        "plt.ylabel('bleu score')\n",
        " \n",
        "# giving a title to my graph\n",
        "plt.title('Graph')\n",
        " \n",
        "# function to show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "j87U13TYmPq-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "outputId": "51ebef26-5a97-4ca7-fd29-2503ec37f8c3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_sentences = []\n",
        "\n",
        "with open('data/test.eng') as targetS:\n",
        "    for line in targetS:\n",
        "        target_sentences.append(line)\n",
        "\n",
        "\n",
        "source_sentences = []\n",
        "\n",
        "with open('data/test.fra') as sourceS:\n",
        "    for line in sourceS:\n",
        "        source_sentences.append(line)\n",
        "\n",
        "\n",
        "\n",
        "test_out = codecs.open('test_beam_1.out','w',encoding='utf-8')\n",
        "\n",
        "beam_size = 4 # optimal beam size calculated in prior step \n",
        "for i,source_sentence in enumerate(source_sentences):\n",
        "\n",
        "    target_sentence = normalizeString(target_sentences[i])\n",
        "    input_sentence = normalizeString(source_sentence)\n",
        "    \n",
        "    hypothesis = beam_search(encoder, attn_decoder, input_sentence, beam_size=beam_size)\n",
        "\n",
        "    test_out.write(hypothesis+'\\n')\n",
        "\n",
        "target.close()\n",
        "test_out.close()    \n"
      ],
      "metadata": {
        "id": "ug88ZlwDcfdy"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!sacrebleu data/test.eng < test_beam_1.out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogW98V37cp0w",
        "outputId": "622b35d6-e06c-4ebb-f27a-bbbf3384212f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sacreBLEU: That's 100 lines that end in a tokenized period ('.')\n",
            "sacreBLEU: It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "sacreBLEU: If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "{\n",
            " \"name\": \"BLEU\",\n",
            " \"score\": 1.6,\n",
            " \"signature\": \"nrefs:1|case:mixed|eff:no|tok:13a|smooth:exp|version:2.0.0\",\n",
            " \"verbose_score\": \"55.6/8.5/0.6/1.2 (BP = 0.378 ratio = 0.507 hyp_len = 3151 ref_len = 6218)\",\n",
            " \"nrefs\": \"1\",\n",
            " \"case\": \"mixed\",\n",
            " \"eff\": \"no\",\n",
            " \"tok\": \"13a\",\n",
            " \"smooth\": \"exp\",\n",
            " \"version\": \"2.0.0\"\n",
            "}\n",
            "\u001b[0m"
          ]
        }
      ]
    }
  ]
}